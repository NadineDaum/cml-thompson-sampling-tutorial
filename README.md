# A Tutorial on Thompson Sampling  
### Based on Russo et al. (2018)

**“Explore when unsure, exploit when sure.”**

This repository contains a self-contained slide presentation explaining **Thompson Sampling (TS)**, one of the most influential algorithms for solving the **multi-armed bandit** problem.  
It is designed to be clear, intuitive, and accessible — while still highlighting the underlying Bayesian principles that make TS powerful.

---

The presentation covers:

- The **exploration–exploitation dilemma**
- The **multi-armed bandit problem** (Bernoulli bandit setup)
- Why **greedy** and **ε-greedy** strategies fail
- **Bayesian updating** of Beta–Bernoulli models
- The intuition behind TS:  
  > *“Pretend the world looks like one posterior draw, act optimally in that world.”*
- Pseudocode and algorithmic intuition  
- Strengths, limitations, and practical applications  
- Open questions & discussion prompts  

It can serve as a teaching resource, a portfolio piece, or a foundation for deeper work in RL & adaptive experimentation.


---

Topics: Multi-Armed Bandits, Bayesian Methods, Thompson Sampling,
Causal ML, Reinforcement Learning, Exploration–Exploitation


---

Reference

Russo, D., Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018).
A Tutorial on Thompson Sampling.
Foundations and Trends® in Machine Learning.
